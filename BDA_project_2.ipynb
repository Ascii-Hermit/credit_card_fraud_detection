{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wmvUXgtypif"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install -q pyspark\n",
        "!pip install -q seaborn\n",
        "!pip install -q matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, log\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5AjdoSjRzARr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set environment variables for PySpark\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--driver-memory 28g pyspark-shell'\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('FraudDetection') \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"32g\") \\\n",
        "    .config(\"spark.executor.memory\", \"32g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load the data\n",
        "path = \"Fraud.csv\"\n",
        "fraud = spark.read.csv(path, header=True, inferSchema=True, multiLine=True, escape='\"')\n",
        "\n",
        "# Count the original number of rows\n",
        "original_count = fraud.count()\n",
        "print(f\"Original count of rows: {original_count}\\n\")\n",
        "\n",
        "# Remove the last 1000 rows\n",
        "if original_count > 636000:\n",
        "    filtered_fraud = fraud.limit(original_count - 636000)\n",
        "else:\n",
        "    print(\"Not enough rows to remove 1000.\\n\")\n",
        "    filtered_fraud = fraud  # Keep all rows if less than 1000\n",
        "\n",
        "# Display the count of rows after filtering\n",
        "filtered_count = filtered_fraud.count()\n",
        "print(f\"Count of rows after removing last 1000: {filtered_count}\\n\")\n",
        "\n",
        "# Checking unique values for nameOrig and nameDest columns\n",
        "print(\"Unique values in 'nameOrig':\", filtered_fraud.select('nameOrig').distinct().count(), \"\\n\")\n",
        "print(\"Unique values in 'nameDest':\", filtered_fraud.select('nameDest').distinct().count(), \"\\n\")\n",
        "\n",
        "# Check for null values\n",
        "filtered_fraud.select([col(c).isNull().alias(c) for c in filtered_fraud.columns]).show()\n",
        "print(\"Checked for null values in each column.\\n\")\n",
        "\n",
        "# Fraud transaction statistics\n",
        "total_count = filtered_fraud.count()\n",
        "legit_transaction = filtered_fraud.filter(filtered_fraud.isFraud == 0).count()\n",
        "fraud_transaction = filtered_fraud.filter(filtered_fraud.isFraud == 1).count()\n",
        "print(\"Transaction statistics completed.\\n\")\n",
        "legit_percent = (legit_transaction / total_count) * 100\n",
        "fraud_percent = (fraud_transaction / total_count) * 100\n",
        "\n",
        "print(f\"Total Legit transactions: {legit_transaction}\\n\")\n",
        "print(f\"Total Fraud transactions: {fraud_transaction}\\n\")\n",
        "print(f\"Percentage of Legit transactions: {legit_percent:.4f}%\\n\")\n",
        "print(f\"Percentage of Fraud transactions: {fraud_percent:.4f}%\\n\")\n",
        "\n",
        "# Analyzing fraud by transaction type\n",
        "grouped_df = filtered_fraud.groupBy('type', 'isFraud').count().toPandas()\n",
        "print(\"Fraud analysis by transaction type:\\n\")\n",
        "print(grouped_df.pivot(index='type', columns='isFraud', values='count'), \"\\n\")\n",
        "\n",
        "# Label Encoding categorical columns\n",
        "indexer = StringIndexer(inputCol=\"type\", outputCol=\"typeIndex\")\n",
        "filtered_fraud = indexer.fit(filtered_fraud).transform(filtered_fraud)\n",
        "\n",
        "indexer_nameDest = StringIndexer(inputCol=\"nameDest\", outputCol=\"nameDestIndex\")\n",
        "filtered_fraud = indexer_nameDest.fit(filtered_fraud).transform(filtered_fraud)\n",
        "\n",
        "# Create new features: diffDist and diffOrg\n",
        "filtered_fraud = filtered_fraud.withColumn(\"diffDist\", col(\"oldbalanceDest\") - col(\"newbalanceDest\"))\n",
        "filtered_fraud = filtered_fraud.withColumn(\"diffOrg\", col(\"oldbalanceOrg\") - col(\"newbalanceOrig\"))\n",
        "print(\"Created new features: 'diffDist' and 'diffOrg'.\\n\")\n",
        "\n",
        "# Ensure non-negative values for log transformation\n",
        "for col_name in ['amount', 'oldbalanceDest', 'newbalanceDest', 'oldbalanceOrg', 'newbalanceOrig']:\n",
        "    min_value = filtered_fraud.agg({col_name: \"min\"}).collect()[0][0]\n",
        "    shift_value = abs(min_value) + 1 if min_value <= 0 else 0\n",
        "    filtered_fraud = filtered_fraud.withColumn(col_name, log(col(col_name) + shift_value))\n",
        "\n",
        "# Drop irrelevant columns\n",
        "filtered_fraud = filtered_fraud.drop('nameOrig', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest')\n",
        "print(\"Dropped irrelevant columns.\\n\")\n",
        "\n",
        "# Assembling features for the model\n",
        "feature_cols = ['amount', 'diffDist', 'diffOrg', 'typeIndex', 'nameDestIndex']\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "filtered_fraud = assembler.transform(filtered_fraud)\n",
        "print(\"Encoded features for model.\\n\")\n",
        "\n",
        "filtered_fraud = filtered_fraud.repartition(1000)\n",
        "\n",
        "# Standardizing features\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "filtered_fraud = scaler.fit(filtered_fraud).transform(filtered_fraud)\n",
        "print(\"Standardized features.\\n\")\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = filtered_fraud.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(labelCol=\"isFraud\", featuresCol=\"scaledFeatures\", maxDepth=3)\n",
        "model = dt.fit(train_data)\n",
        "\n",
        "# Predictions\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluation metrics\n",
        "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"isFraud\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator_accuracy.evaluate(predictions)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\\n\")\n",
        "\n",
        "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"isFraud\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "precision = evaluator_precision.evaluate(predictions)\n",
        "print(f\"Model Precision: {precision:.4f}\\n\")\n",
        "\n",
        "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"isFraud\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "recall = evaluator_recall.evaluate(predictions)\n",
        "print(f\"Model Recall: {recall:.4f}\\n\")\n",
        "\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"isFraud\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator_f1.evaluate(predictions)\n",
        "print(f\"Model F1 Score: {f1_score:.4f}\\n\")\n",
        "\n",
        "# Confusion Matrix\n",
        "y_true = predictions.select(\"isFraud\").toPandas()\n",
        "y_pred = predictions.select(\"prediction\").toPandas()\n",
        "\n",
        "# Generate confusion matrix and display it\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification report:\\n\")\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "# Save models and scalers\n",
        "indexer.write().overwrite().save(\"type_indexer\")\n",
        "indexer_nameDest.write().overwrite().save(\"nameDest_indexer\")\n",
        "assembler.write().overwrite().save(\"assembler\")\n",
        "scaler.write().overwrite().save(\"scaler\")\n",
        "model_path = \"fraud_detection_model\"\n",
        "model.save(model_path)\n",
        "print(f\"Models and transformers saved. Model path: {model_path}\\n\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "rVqq_ZIx20BB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}